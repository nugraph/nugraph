{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749149dd-b4f2-4fb8-9897-9cadd42128d8",
   "metadata": {},
   "source": [
    "### Set autoreloading\n",
    "This extension will automatically update with any changes to packages in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1020db2-2ee5-49cb-99ac-fb28a348e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e31949-fb8c-437d-a7e1-775679d0730b",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "We'll need the `pytorch_lightning` and `nugraph` packages imported in order to run inference, the `time` package to measure inference time, and `plotly.graph_objects` to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1b7ab-1388-4ef3-bb25-356bdd0c5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import nugraph as ng\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec59ca-e71c-4cf9-8c5b-42fcf8bd0811",
   "metadata": {},
   "source": [
    "### Set default plotting options\n",
    "\n",
    "Define a dictionary containing all standard plotting options that we want to set for all the plots we draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b78655-7e40-47d0-8b04-f98df02b3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {\n",
    "    'layout_width': 800,\n",
    "    'layout_height': 450,\n",
    "    'layout_margin_b': 20,\n",
    "    'layout_margin_t': 20,\n",
    "    'layout_margin_r': 20,\n",
    "    'layout_margin_l': 20,\n",
    "    'layout_xaxis_title_font_size': 24,\n",
    "    'layout_xaxis_tickfont_size': 20,\n",
    "    'layout_yaxis_title_font_size': 24,\n",
    "    'layout_yaxis_tickfont_size': 20,\n",
    "    'layout_legend_font_size': 24,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1facb-608e-4543-b03e-cabb7de2612a",
   "metadata": {},
   "source": [
    "### Configure network\n",
    "In order to test a trained model, we instantiate it using a checkpoint file. These are produced during training, so if you've trained a model, there should be an associated checkpoint in your output directory that you can pass here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05ed20-20e8-46f8-89d4-1657e13ee52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ng.models.NuGraph2.load_from_checkpoint('/raid/uboone/NuGraph2/NG2-paper', map_location='cpu')\n",
    "model.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e894927-4bdc-4743-b335-9ef628df9f9c",
   "metadata": {},
   "source": [
    "### Benchmark inference time\n",
    "Loop over a range of batch size options. For each one, run testing and record how long it took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b06978-7216-4735-861b-7aa594094881",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator, devices = ng.util.configure_device()\n",
    "x = []\n",
    "y = []\n",
    "for i in range(9):\n",
    "    batch_size = pow(2, i)\n",
    "    x.append(batch_size)\n",
    "    nudata = ng.data.H5DataModule(\n",
    "        data_path='/raid/uboone/NuGraph2/NG2-paper.gnn.h5',\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    accelerator, devices = ng.util.configure_device()\n",
    "    trainer = pl.Trainer(accelerator=accelerator,\n",
    "                         devices=devices, logger=False)\n",
    "    t0 = time.time()\n",
    "    trainer.test(model, datamodule=nudata)\n",
    "    y.append((time.time()-t0)/len(nudata.test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5220a-8107-4312-a9e9-29421c55813e",
   "metadata": {},
   "source": [
    "### Plot inference time\n",
    "Draw a scatter plot using the batch sizes and inference times from the previous step. Save the resulting plot to disk and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d84b5-7ac5-4965-9f72-5c9810c20f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    layout_xaxis_title='Batch size',\n",
    "    layout_yaxis_title='Inference time per graph [s]',\n",
    "    layout_xaxis_type='log',\n",
    "    layout_xaxis_tickmode='array',\n",
    "    layout_xaxis_ticktext=x,\n",
    "    layout_xaxis_tickvals=x,\n",
    "    **style,\n",
    ")\n",
    "fig.add_scatter(x=x, y=y)\n",
    "\n",
    "fig.write_image('plots/inference-time.png')\n",
    "fig.write_image('plots/inference-time.pdf')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972cb170-9523-462f-8309-e54849832e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
